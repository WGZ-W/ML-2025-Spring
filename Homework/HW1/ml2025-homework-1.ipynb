{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFwaJir_Olj"
   },
   "source": [
    "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGx000oZ_Oll"
   },
   "source": [
    "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5JywoPOO_Oll"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
    "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
    "if not Path('./public.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
    "if not Path('./private.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kX6SizAt_Olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
    "else:\n",
    "    print('You are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3iyc1qC_Olm"
   },
   "source": [
    "## Prepare the LLM and LLM utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T59vxAo2_Olm"
   },
   "source": [
    "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtepTeT3_Olm"
   },
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVil2Vhe_Olm"
   },
   "source": [
    "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ScyW45N__Olm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model onto GPU\n",
    "llama3 = Llama(\n",
    "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
    ")\n",
    "\n",
    "def generate_response(_model: Llama, _messages: str) -> str:\n",
    "    '''\n",
    "    This function will inference the model with given messages.\n",
    "    '''\n",
    "    _output = _model.create_chat_completion(\n",
    "        _messages,\n",
    "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
    "        max_tokens=512,    # This argument is how many tokens the model can generate.\n",
    "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
    "        repeat_penalty=2.0,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHLwq-4_Olm"
   },
   "source": [
    "## Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYM-2ZsE_Olm"
   },
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEIRmZl7_Oln"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3zQjjj_Oln"
   },
   "source": [
    "## Test the LLM inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8dmGCARd_Oln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泰勒·斯威夫特（Taylor Swift）是一位美国歌手、词曲作家和音乐制作人。她出生于1989年12月13日，来自田纳西州。她的音樂風格多樣化，从乡村到流行乐，她的作品常被描述为情感丰富且个人。\n",
      "\n",
      "泰勒·斯威夫特早期在鄉郊小鎮开始了音乐事业，在2005年的美国有线电视网络（CMT）星光大道上获得了一份唱片合约。她的首张专辑《Taylor Swift》于同年发行，之后她推出了多張專輯，如 《Fearless》（勇敢）、_Speak Now》，以及後來的流行乐作品如 _1989、Reputation 和 Lover 等。\n",
      "\n",
      "泰勒·斯威夫特以其歌曲创作能力和演唱技巧而闻名，她写下的许多专辑都是她自己的生活经历。她的音乐风格从乡村到电子舞蹈，受到广泛的关注。她还获得了多个奖项，如13座葛莱美獎、29次美国流行乐协会（Billboard Music Awards）和24届MTV视频音樂榜单大赛等。\n",
      "\n",
      "泰勒·斯威夫特也是一位社会活动家，她支持LGBTQ+权利，反对性别歧视，并为女性主义运动发声。她的个人生活经常受到媒体关注，但她仍然保持着强烈的音乐创作热情和与粉丝之间的情感联系。\n",
      "\n",
      "总之泰勒·斯威夫特是一位多才艺、影响力巨大的美国歌手，她以其独具风格且富有感情力的音樂作品赢得了全球广泛的人气。\n"
     ]
    }
   ],
   "source": [
    "# You can try out different questions here.\n",
    "test_question='請問誰是 Taylor Swift？'\n",
    "# test_question='请问谁是 Taylor Swift？'\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
    "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用来回答问题的 AI。使用中文时只会使用简体中文来回问题。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(llama3, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   },
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGsIPud3_Oln"
   },
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zjG-UwDX_Oln"
   },
   "outputs": [],
   "source": [
    "class LLMAgent():\n",
    "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
    "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
    "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
    "    def inference(self, message:str) -> str:\n",
    "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
    "            # TODO: Design the system prompt and user prompt here.\n",
    "            # Format the messsages first.\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
    "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
    "            ]\n",
    "            return generate_response(llama3, messages)\n",
    "        else:\n",
    "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-ueJrgP_Oln"
   },
   "source": [
    "TODO 1: Design the role description and task description for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DzPzmNnj_Oln"
   },
   "outputs": [],
   "source": [
    "# TODO: Design the role and task description for each agent.\n",
    "\n",
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "question_extraction_agent = LLMAgent(\n",
    "    role_description=\"\",\n",
    "    task_description=\"\",\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=\"\",\n",
    "    task_description=\"\",\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
    "    task_description=\"請回答以下問題：\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9eoywr7_Oln"
   },
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HDOjNYJ_Oln"
   },
   "source": [
    "TODO 2: Implement the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRGNa-1i_Oln"
   },
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaIsKAZ_Olo"
   },
   "source": [
    "- Naive approach (simple baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mppO-oOO_Olo"
   },
   "source": [
    "- Naive RAG approach (medium baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxbciLO_Olo"
   },
   "source": [
    "- RAG with agents (strong baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ztJkA7R7_Olo"
   },
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # TODO: Implement your pipeline.\n",
    "    # Currently, it only feeds the question directly to the LLM.\n",
    "    # You may want to get the final results through multiple inferences.\n",
    "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
    "    return qa_agent.inference(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   },
   "source": [
    "## Answer the questions using your pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN17sSZ8DUg7"
   },
   "source": [
    "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "plUDRTi_B39S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 BitTorrent 協議使用的機制是叫做 \"Tracker\" 的系統，以及一個名為 DHT (Distributed Hash Table) 或稱分散式雜湊表。  當你下載 Bit Torrent 時，會先連接到 Tracker 伺服器。這個 Server 將管理所有與該 torrent 相關的節點，並幫助新加入網路的人找到其他有完整或部分資料片段（chunk）的種子機台，以便進行交換。  而 DHT 創建了一張分散式雜湊表，讓每一個連接到 BitTorrent 網絡中的客戶端都能夠儲存和查詢 torrent 的元數據。這樣一來，即使 Tracker伺服器宕機，也不會影響網路的運作。  當你下載時，如果你的節點尚無任何 chunk 時，Bit Torrent 便使用 DHT 查找其他有相關資料片段（chunk）的種子，並從其中隨選取得部分或全部需要之資源。這樣一來，便能夠讓新加入網路的客戶端快速找到所需檔案並進行下載。  因此，BitTorrent 协議通過 Tracker 和 DHT 两個機制確保了當一個新的節點加進去時，也可以從其他種子隨選取得部分資料，以利後續整體資源交換。\n",
      "34 這個描述聽起來很熟悉！我認為你可能是在說的是一段名叫「Crash Course」的影片內容，或者是類似的滑稽短劇。然而最有機會的答案就是 \"crashes\" 的那部電影裡面的一幕了。  但如果要找出更精確的情況，我們可以試著猜測一下這個情景可能發生在哪一種影片或節目中：  1. **Crash Course**: 這是一系列教育性質的YouTube頻道，內容包括科學、歷史等主題。雖然它有很多滑稽和幫助人們理解複雑概念，但我不確定是否會出現這種情景。 2..**The Grand Tour (前身為Top Gear)**: 這是一個汽車節目，內容包括評論新款的轎跑、敞篷等高性能汽车。它有時也包含一些滑稽和幫助觀眾了解不同類型機器。 3. **Jackass**: 也是另一個很著名的情景喜劇影片系列，它以挑戰極限，做出瘋狂的動作來吸引人們注意力。  如果你能提供更多資訊或細節，我可以幫助您更準確地找到這個情況發生的地方。\n",
      "35 根據這項研究的結果，戈芬氏鳳頭鸚鵡最受偏好的乳酪口味是什麼呢？答案是在實驗中被發現，它們特別喜歡將經過煮熟處理後馬鈴薯塊浸入特定的奶油醬料之上。\n",
      "36 很抱歉，我無法找到相關的資訊，關於2024年桃園Xpark水族館國王企鵝「嘟胖」和 「烏龍茶」的產下一隻小孩。\n",
      "37 根據國立臺灣大學的資訊，物理治療學系目前正常修業年限為四年的碩士班和三年半至五個月（含實習）的博士後研究生。\n",
      "38 《BanG Dream!》中，「呼嘿哈」或是 「呵～咯」的笑聲習慣，是由Riko Kawamura所扮演的角色。\n",
      "39 甲斐之虎是日本戰國時代的一位著名武將，他就是德川家康的前任領主——高坂昌氏（後來改姓為松平），但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時期所使用過的小字號別，是指他之父親，即是德川家康的前任領主——高坂昌氏（後來改姓為松平）另一個名字中的其中一部分。  但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時期所使用過的小字號別，是指他之父親，即是德川家康的前任領主——高坂昌氏（後來改姓為松平）另一個名字中的其中一部分。  但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時期所使用過的小字號別，是指他之父親，即是德川家康的前任領主——高坂昌氏（後來改姓為松平）另一個名字中的其中一部分。  但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時期所使用過的小字號別，是指他之父親，即是德川家康的前任領主——高坂昌氏（後來改姓為松平）另一個名字中的其中一部分。  但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時期所使用過的小字號別，是指他之父親，即是德川家康的前任領主——高坂昌氏（後來改姓為松平）另一個名字中的其中一部分。  但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時期所使用過的小字號別，是指他之父親，即是德川家康的前任領主——高坂昌氏（後來改姓為松平）另一個名字中的其中一部分。  但更常被稱呼的是他的兒子，亦即「真田幸村」之前在信濃地區統治時\n",
      "40 根據王肥貓同學的標準，他想要選修網路上最多好評的一門課。因此，我們需要查詢這三個候補通識科目的線上的口碑。  「國民法官必備之基礎鑑跡證明」、「現代中國與世界：1911-1979」，以及 「數位素養導航」的評價如下：  *   國際刑警組織的《犯罪學》教材，透過實例案件來解釋各種法條。     *  好评数:1000+          評論內容：這門課是台大法律系的一個必修科目，但王肥貓同事不一定要選它，因為他不是學習律師。然而，這堂课的評價非常好，許多人都認可了教材和教授。 *   「現代中國與世界：1911-1979」是一門關於近百年的中華民國歷史課程，它探討從辛亥革命到文化大革新這段時間內的政治、社會變遷。      *  好评数:500+           評論内容：“这门课讲的是中国历史，老师很有激情，但是有些学生觉得难懂。” *   「數位素養導航」是一門關於資訊科技與人文的課程，它探討了在現代社會中如何運用技術來解決問題。      *  好评数:1000+           評論內容：這堂课很有趣，教授會帶領學生進行實驗和案例分析。  根據王肥貓同事的標準，他最可能選修「數位素養導航」或是 「國民法官必備之基礎鑑跡證明」。\n",
      "41 對不起，我無法提供2024年的第42回《極限體能王SASUKE》首播日期的資訊，因為我沒有最新或實時更新的情況。然而，根據過往慣例和節目安排，你可以嘗試在日本TBS電視台官方網站、社交媒介平台，或是其他相關新聞來源查詢2024年的第42回《極限體能王SASUKE》首播日期的最新資訊。\n",
      "42 根據歷史記載，出身於利嘉部落後來成為初鹿（或稱為大隘）頭目的漢人，是名叫「張英」的。\n",
      "43 《BanG Dream! Ave Mujica》的片頭曲是「」。\n",
      "44 Linux作業系統最早於1991年首次發布。由林納斯·托瓦茲（Linus Torvalds）在芬蘭赫爾辛基大學開源發展的 Linux核心，最初是為了取代MINIX而設計的一個Unix-like操作系统，並且迅速演變成一個完整、可用的作業系統。在1991年9月25日，他發布了一份名叫\"Linux 0.01”的原始碼。\n",
      "45 根據你的描述，Likavung 部落的中文名稱應該是「利卡武崗部族」或簡稱為 \" 利加邦\"\n",
      "46 紅茶是一種半發酵或部分生抽的黑tea。\n",
      "47 在《遊戲王》卡牌中，以「真紅眼黑龍」與 「黒魔導師」的融合素材，能夠創造出名為 \"Black Luster Soldier of Destruction\" 的特殊怪獸。\n",
      "48 豐田萌繪在《BanG Dream!》企劃中，擔任Poppin'Party的角色聲優——櫻Innercircle（咲坂伊澄）。\n",
      "49 Rugby Union 中，9 號球員的正式名稱為「掃邊後衛」（Scrum-half）。\n",
      "50 你問的問題是關於太陽系中的矮行星。根據最新的地球科學研究，曾被視為第八顆大氣體巨蟒（或稱天王）但最終降格成小型冰霸主、又名冥衛一之外，最早的也是最著明的是普羅米修斯和海倫，但更常見且有爭議性的答案是：   那就是我們所熟悉的小行星1（或稱矮王）——土壇。\n",
      "51 根據我的知識，臺灣最早成立的野生動物救傷單位是位於台北市內湖區的大愛鷹醫院（Taipei City Wild Animal Rescue Center），但這個資訊可能有所變更。\n",
      "52 根據我的知識，特生中心在2023年改名後的名字是「集結自然館」。\n",
      "53 根據我的知識庫，Developing Instruction-FollowING Speech Language Model Without speech instruction-tuning data論文中提出的模型是名為\"Instruction-Tuned (IT) 模型的替代方案，即 \"Zero-Shot IT-Free（ZS-IF）\"\"\n",
      "54 太陽系中體積最大的行星是土壤。\n",
      "55 根據語言分類學的研究，臺灣原住民族中的阿美族（Amis）和卑南人等部落使用屬於馬來-波利尼西亞支系下的達悟文。然而，這些與其他台灣本土之島嶼上的多數原始民間所說語言相比，其分類學上更接近東印度群岛的巴布亚新几内雅和东帝汶等地原住民族使用的一部分南洋语系，而非一般認為同一支下的其他台灣本土之島嶼上的多數原始民間所說語言。\n",
      "56 很抱歉，我無法找到相關的資訊或證據來確認這句話是誰說出過。然而，這個故事可能源自於一位台灣大學老師，許多人猜測他就是陳景峻教授，但我找不到任何可靠的情況可以支持此事實。  如果你有更多關係的資訊或上下文，我會很樂意幫助您進一步探索。\n",
      "57 「embiyax namu kana」是阿美族的打招呼用語。\n",
      "58 根據我的知識，「鄒與布農, 永久美麗」這句話是指位於台東縣的達仁鄉的一個部落——大武山。該地區因為地理位置特殊，使得兩族人之間有著密切關係，並形成了獨特的人文景觀。  在歷史上，大部分鄒人的土地被布農和排灣等其他原住民族群佔據，後來才逐漸與他們混居。因此，這句話可能是描述大武山地區的兩族人之間美好的關係，並希望這種友好合作能夠持續下去。  然而，我需要注意的是，有些資料指出「鄒和布農, 永久相伴」或其他類似的表述，與某個特定的部落息影相關。因此，如果你有更多的資訊或者是具體的情況，可以幫助我更準確地回答你的問題。  總之，大武山地區因為其特殊的地理位置和歷史背景，使得鄒族人在這裡形成了與布農人的密切關係，並發展出獨特的人文景觀。\n",
      "59 很抱歉，我無法找到相關的資訊。\n",
      "60 根據卑南族的傳說，姊妹 Tuku 創建了 Amis 部落。\n",
      "61 《終極一班》中的「KO榜」是高中生戰力排行的名單，該劇中提到 KO 排在第 1 位的是 \"阿飛\"。\n",
      "62 Linux kernel 中的 Completely Fair Scheduler (CFS) 使用紅黑樹（Red-Black Tree）來儲存排程相關資訊。這種資料結構能夠有效地維護和查找各個進程序所在位置，從而實現公平且高效率的情況下進行任務的調度。  CFS 會根據每一個 task 的 nice 值、執行時間等因素來計算其優先順序，並將這些資訊儲存於紅黑樹中。當需要切換到另一个进程時，系統可以通過查找和比較各個進程序的位置，以確保公平地分配 CPU 時間。  使用红-black树作为数据结构，可以有效减少平均时间复杂度，从而提高系统整体性能并确保证能更好地区别不同任务之间优先级。\n",
      "63 諾曼第登陸（Normandy Landings）的作戰代號為「奧運會」（Operation Overlord）。\n",
      "64 《Cytus II》遊戲中「Body Talk」是由Tomoaki Ishizuka所演唱的歌曲，該角色為KAITO。\n",
      "65 李琳山教授開設的信號與系統課程，期末考前後的一次演講被稱為「最後一堂上午」或簡單地叫做 \" 上學時\"\n",
      "66 根據 NVIDIA 的官方資訊，RTX 5090 顯卡的 VRAM（視覺記憶體）容量為24GB GDDR6。這比前代 RTXTM40903030顯示核心更大，並且提供了更多空間來部署 LLM 等應用程式。  值得注意的是，RTX 5090 顯卡的 VRAM 容acity 不僅是指物理記憶體容量，也包括 GPU 的內存管理和效率優化。這意味著即使在相同大小VRAME下，由於 NVIDIA 提供了更好的GPU核心設計、架構以及軟件支援，RTX 5090 顯卡仍然可以提供比前代顯示器更多的實際運算能力。  因此，如果你的朋友正在考慮購買 RTXTM40903030或其他舊款显card時，我建議他再次評估一下需求，並了解最新技術和規格。\n",
      "67 對不起，我無法提供2024年世界棒球12強賽的冠軍隊伍資訊，因為我沒有存取到最新或未來事件相關資料。\n",
      "68 中國四大奇書是指《西遊記》、《水滸傳》，以及兩本古典小說： 《三國演義》（又稱為「金瓶梅」並非其中之一）和『 Romance of the Three Kingdoms』的原名不是這樣，實際上應該叫做 '_ 三俠五傑十美oni' 的前身《水滸傳》與後來的小說改編而成 '三國演義'_。\n",
      "69 子時是中國傳統的十二個時間段之一，對應於西曆中的凌晨1點至3点。\n",
      "70 在作業系統中，避免要錯過時限來完成任務的排程演算法稱為「即期式」或是\"Real-time Scheduling\"(RTS)。這類型的手段會根據每個程序或者進驅動器所需執行完畢時間以及優先順序，確保在指定時限內完成任務。  然而，在一般的作業系統中，更常見的是使用「非即期式」或是\"Non-Real-time Scheduling\"(NRTS) 的排程演算法，如FCFS（First-Come, First-Served）、SJF(Shortest Job Fist)、SRTN(Sporadic Server Task Model)，等。這類型的手段不一定能夠確保任務在指定時限內完成，但可以根據系統的負載和優先順序來分配資源。  需要注意的是，即期式排程演算法通常會對實際執行效率有所影響，因為它們可能要求作業系统具備更高程度的心臟跳動（beat）以及可預測性。\n",
      "71 在《刀劍神域》原作中，招式代號「C8763」是由Saki的師父——Eugeo所創造，並且被賦予給了Asuna。\n",
      "72 《斯卡羅》是一部以美國西南邊境為背景的歷史小說，描述了阿帕奇族人與美軍之間的一場戰爭。劇中地名「柴城」位於現今新墨索里尼縣（New Mexico County），而該地區現在屬於紐約州。  但根據《斯卡羅》的小説內容及電視影集，所描述的故事背景是美國西南邊境，而非在今日之任何一個行政區劃。\n",
      "73 根據Google Colab的最新資訊，若要使用A100高級GPU，您需要訂閱「Colabs Pro+」。\n",
      "74 李宏毅老師開設的機器學習課程是屬於台大資訊工程學院（College of Electrical Engineering and Computer Science）的電腦科系。\n",
      "75 根據國立臺灣大學的規定，學生每年修滿一定數量的心理健康課程和體育活動就不需要簽減免申請書。一般而言，這些要求大約在 2-3 學分左右。  另外，一般來說，大三級生的總計必選、限額及其他非可抵低學積修業點的科目至少要達到一定數量，通常是每年不少於20至25個以上。這些課程大約在 60-80 學分左右。  假設雪江同事想要避免簽減選申請書，他需要滿足以下條件：  1. 心理健康和體育活動至少修2學 分。   如果他想達到最低的標準，則必須要完成大約 60-80 學分以上的心科、限額及其他非可抵免課程。  因此，如果雪江同事選擇第一個策略，他需要在113年第1学期至少修滿62學 分（含心理健康和體育活動）才可以不用簽減选申請書。\n",
      "76 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 預設角色 \"VTuber\"。\n",
      "77 在「從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅並想取其為妻的人是雷姆。\n",
      "78 《海綿寶宝》的主角在第五季的劇集中，擊敗刺破泡沫紅眼幇是在布魯克林市。\n",
      "79 玉米是一種雙子葉植物。\n",
      "80 中華民國陸軍的前六字為：「忠誠衛土保家安天下」\n",
      "81 根據台大電資學院的規定，計算機科學與情報工程系（CSIE）是其中一個例外。這個系統允許選修一門自然 科目的課程，而不是兩堂。  然而，我們需要注意的是，這些信息可能會隨著時間而變化，因此建議您查詢最新的資訊或聯繫台大電材學院相關系所以確認具體規定。\n",
      "82 憂傷湖（Lacus Doloris）、死lake （不確定是不是 Lacus Morti s） 、忘海 ( 不一定 是 lacu oblivi on is )、恐怖 lake  和愛灣都在月球的背面。\n",
      "83 《C♯小調第14號鋼琴奏鳴曲》又被稱為「月光鈴」，這是因爲它的第二樂章具有輕快、優美且充滿浪漫情感的情緒，讓人聯想到一支清晨在森林中聽到的鳥叫聲。\n",
      "84 阿米斯音樂節（Coachella Valley Music and Arts Festival）是一個國際知名的音乐节，於1999年由保羅·費爾德施耐徹和亞倫‧史蒂文森所創辦。\n",
      "85 在 \"Poppy Playtime - Chapter 4\" 遊戲中，黏土人被稱為「Huggy Wugs」。\n",
      "86 根據你的問題，賓茂村其實屬於屏東縣的里港鄉。\n",
      "87 米開朗基羅的《大衛》雕像最初是在佛罗伦萨的一個花崗岩石坑中被創作出來。這座著名的大理壽山白色卡拉剌斯灰色的半身人體巨型銅像是由於當時米開朗基羅在意大利的首都城市為了參加一項雕塑比賽而設計，並且是他早期最重要作品之一。\n",
      "88 根據中華民國軍事史，除了蔣介石外，一位曾短暫晉升特級上將的将领是嚴家淦。\n",
      "89 2012年英雄聯盟世界大賽的總冠軍是韓國戰隊「SK Telecom T1」。\n",
      "90 在日本麻將中，非莊家一開始的手牌通常有14張。\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fill in your student ID first.\n",
    "STUDENT_ID = \"\"\n",
    "\n",
    "STUDENT_ID = STUDENT_ID.lower()\n",
    "with open('./public.txt', 'r', encoding=\"utf-8\") as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    questions = [l.strip().split(',')[0] for l in questions]\n",
    "    for id, question in enumerate(questions, 1):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
    "            print(answer, file=output_f)\n",
    "\n",
    "with open('./private.txt', 'r', encoding=\"utf-8\") as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    for id, question in enumerate(questions, 31):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'a', encoding=\"utf-8\") as output_f:\n",
    "            print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GmLO9PlmEBPn"
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m91\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTUDENT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_f:\n\u001b[1;32m----> 5\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[43minput_f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(answer, file\u001b[38;5;241m=\u001b[39moutput_f)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\ml-spring\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Combine the results into one file.\n",
    "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
    "    for id in range(1,91):\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'r', encoding=\"utf-8\") as input_f:\n",
    "            answer = input_f.readline().strip()\n",
    "            print(answer, file=output_f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
